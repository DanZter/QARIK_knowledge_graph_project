{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT__sentiment_model_with_flaskAPI (TPU)","provenance":[],"collapsed_sections":[],"mount_file_id":"1HsKL2CgSbQh88HAecIVSCnH1VTOgjgTZ","authorship_tag":"ABX9TyMagR1g4ZCT2nVUonWMsl4E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"nhycw09090GQ","colab_type":"code","colab":{}},"source":["import os\n","assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YmP46Y6-DBG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594758530313,"user_tz":-60,"elapsed":70091,"user":{"displayName":"DanZter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggkfyqMkVJskLrFMxW8D7v1oM_mQxB-brWu1mfLw=s64","userId":"10290021482994430772"}},"outputId":"a7a0a7ba-46dc-442f-8b13-ec459aef7207"},"source":["VERSION = \"1.5\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n","!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","!python pytorch-xla-env-setup.py --version $VERSION"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  5115    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  58125      0 --:--:-- --:--:-- --:--:-- 57471\n","Updating... This may take around 2 minutes.\n","Updating TPU runtime to pytorch-1.5 ...\n","Collecting cloud-tpu-client\n","  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n","Collecting google-api-python-client==1.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 2.6MB/s \n","\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.12.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (49.1.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n","Uninstalling torch-1.5.1+cu101:\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n","Installing collected packages: google-api-python-client, cloud-tpu-client\n","  Found existing installation: google-api-python-client 1.7.12\n","    Uninstalling google-api-python-client-1.7.12:\n","      Successfully uninstalled google-api-python-client-1.7.12\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n","Done updating TPU runtime\n","  Successfully uninstalled torch-1.5.1+cu101\n","Uninstalling torchvision-0.6.1+cu101:\n","  Successfully uninstalled torchvision-0.6.1+cu101\n","Copying gs://tpu-pytorch/wheels/torch-1.5-cp36-cp36m-linux_x86_64.whl...\n","- [1 files][ 79.0 MiB/ 79.0 MiB]                                                \n","Operation completed over 1 objects/79.0 MiB.                                     \n","Copying gs://tpu-pytorch/wheels/torch_xla-1.5-cp36-cp36m-linux_x86_64.whl...\n","- [1 files][106.6 MiB/106.6 MiB]                                                \n","Operation completed over 1 objects/106.6 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torchvision-1.5-cp36-cp36m-linux_x86_64.whl...\n","/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n","Operation completed over 1 objects/2.5 MiB.                                      \n","Processing ./torch-1.5-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (0.16.0)\n","\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n","Installing collected packages: torch\n","Successfully installed torch-1.5.0a0+ab660ae\n","Processing ./torch_xla-1.5-cp36-cp36m-linux_x86_64.whl\n","Installing collected packages: torch-xla\n","Successfully installed torch-xla-1.5\n","Processing ./torchvision-1.5-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (7.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.5.0a0+ab660ae)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==1.5) (0.16.0)\n","Installing collected packages: torchvision\n","Successfully installed torchvision-0.6.0a0+3c254fb\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  libomp5\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 234 kB of archives.\n","After this operation, 774 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n","Fetched 234 kB in 1s (380 kB/s)\n","Selecting previously unselected package libomp5:amd64.\n","(Reading database ... 144465 files and directories currently installed.)\n","Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n","Unpacking libomp5:amd64 (5.0.1-1) ...\n","Setting up libomp5:amd64 (5.0.1-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yIqq518C8HSg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"status":"error","timestamp":1594841217787,"user_tz":-60,"elapsed":77202,"user":{"displayName":"DanZter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggkfyqMkVJskLrFMxW8D7v1oM_mQxB-brWu1mfLw=s64","userId":"10290021482994430772"}},"outputId":"b9f4041d-eec4-48fe-9560-81f9f6fdc5ca"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 237\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed: timeout during initial read of root folder; for more info: https://research.google.com/colaboratory/faq.html#drive-timeout"]}]},{"cell_type":"code","metadata":{"id":"FhqBMJ_g-DDt","colab_type":"code","colab":{}},"source":["# imports pytorch\n","import torch\n","\n","# imports the torch_xla package\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.distributed.parallel_loader as pl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkiDlzEt-DGe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":622},"executionInfo":{"status":"ok","timestamp":1594758532659,"user_tz":-60,"elapsed":72401,"user":{"displayName":"DanZter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggkfyqMkVJskLrFMxW8D7v1oM_mQxB-brWu1mfLw=s64","userId":"10290021482994430772"}},"outputId":"4091b6b8-eb81-4175-dc68-07682ab3e0d9"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.2.0\n","Running on TPU  ['10.18.220.146:8470']\n","INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3eAyegtu-DN9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":941},"executionInfo":{"status":"ok","timestamp":1594758556733,"user_tz":-60,"elapsed":96437,"user":{"displayName":"DanZter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggkfyqMkVJskLrFMxW8D7v1oM_mQxB-brWu1mfLw=s64","userId":"10290021482994430772"}},"outputId":"7c101a1b-dceb-44ad-b67d-8e44f69dc5ed"},"source":["!pip install torch\n","!pip install transformers\n","!pip install pandas\n","!pip install -U scikit-learn"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0a0+ab660ae)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 15.5MB/s \n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 24.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 35.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ba3019cf736514c44a26f5c53bf426e44179d9dc63fc9ecfb6ba960e6e052b41\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 3.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CHk_Kx2G-DS_","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","from sklearn import model_selection\n","from scipy import stats\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","# import logging\n","# logging.basicConfig(level=logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2m9k63w1-Dae","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":438},"executionInfo":{"status":"error","timestamp":1594758558166,"user_tz":-60,"elapsed":97859,"user":{"displayName":"DanZter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggkfyqMkVJskLrFMxW8D7v1oM_mQxB-brWu1mfLw=s64","userId":"10290021482994430772"}},"outputId":"8c0955f9-9656-45de-f87e-806ef7e4c564"},"source":["MAX_LEN = 256\n","# TRAIN_BATCH_SIZE = 8\n","# VALID_BATCH_SIZE = 4\n","# EPOCHS = 10\n","# ACCUMULATION = 2\n","BERT_PATH = \"/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased\"\n","TRAINING_FILE = \"/content/drive/My Drive/Colab Notebooks/input/IMDB Dataset.csv\"\n","TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n","MODEL_PATH =\"/content/drive/My Drive/Colab Notebooks/models/bert_sentiment_model.bin\"\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-09b93e1c12ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mBERT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mTRAINING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/input/IMDB Dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mTOKENIZER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/models/bert_sentiment_model.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \"\"\"\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m                 )\n\u001b[1;32m   1248\u001b[0m             )\n","\u001b[0;31mOSError\u001b[0m: Model name '/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed '/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."]}]},{"cell_type":"code","metadata":{"id":"rhm3SbLA-DYQ","colab_type":"code","colab":{}},"source":["class BERTDataset:\n","    def __init__(self, review, target):\n","        self.review = review                     # the review text, a list\n","        self.target = target                     # 0 or 1, a list\n","        self.tokenizer = TOKENIZER\n","        self.max_len = MAX_LEN\n","\n","    def __len__(self):                           # returns the total length of data set\n","        return len(self.review)\n","\n","    def __getitem__(self, item):                 # takes an 'item' and returns tokenizer of that item from data set\n","        review = str(self.review[item])          # converts everything to str incase there exists numbers etc.\n","        review = \" \".join(review.split())        # removes all unnecessary space\n","\n","        inputs = self.tokenizer.encode_plus(     # encode_plus can encode 2 strings at a time\n","            review,\n","            None,                                # since we use only 1 string at a time\n","            add_special_tokens=True,             # adds cld, sep tokens\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            truncation=True\n","        )\n","\n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"] # since only 1 string token_type_ids are same and unnecessary\n","\n","        padding_length = self.max_len - len(ids)  # for bert we pad on the right side\n","        ids = ids + ([0] * padding_length)        # zero times the padding length\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'target': torch.tensor(self.target[item], dtype=torch.float)\n","        }\n","    \"\"\" if we have 2 target outputs then set to torch.long,\n","    depends on loss function also, from cross-entropy we should use torch.long\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7baJs1kv-DV3","colab_type":"code","colab":{}},"source":["def loss_fn(outputs, target):\n","    return nn.BCEWithLogitsLoss()(outputs, target.view(-1, 1))\n","\n","def train_fn(data_loader, model, optimizer, device, scheduler=None):\n","    model.train()\n","\n","    for bi, d in enumerate(data_loader):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        target = d[\"target\"]\n","\n","        ids = ids.to(device, dtype=torch.long)              # send to TPU device\n","        mask = mask.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        target = target.to(device, dtype=torch.float)\n","\n","        optimizer.zero_grad()\n","        outputs = model(\n","            ids=ids,\n","            mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","\n","        loss = loss_fn(outputs, target)        # find loss\n","        loss.backward()                         # backward propagation\n","\n","        xm.optimizer_step(optimizer)\n","        if scheduler is not None:\n","            scheduler.step()\n","        if bi % 10 == 0:\n","            xm.master_print(f\"batch_index={bi}, loss={loss}\")\n","\n","        \"\"\" stop the optimizer only after a certain number of accumulation steps \"\"\"\n","\n","\n","def eval_fn(data_loader, model, device):\n","    model.eval()\n","    fin_target = []                         # final targets\n","    fin_outputs = []                        # final outputs\n","    with torch.no_grad():\n","        for bi, d in enumerate(data_loader):\n","            ids = d[\"ids\"]\n","            mask = d[\"mask\"]\n","            token_type_ids = d[\"token_type_ids\"]\n","            target = d[\"target\"]\n","\n","            ids = ids.to(device, dtype=torch.long)              # send to cuda device\n","            mask = mask.to(device, dtype=torch.long)\n","            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","            target = target.to(device, dtype=torch.float)\n","\n","            outputs = model(\n","                ids=ids,\n","                mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","            # loss = loss_fn(outputs, targets)        # find loss, its bettwer to evaluate loss in eval fn\n","\n","            fin_target.extend(target.cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","    return fin_outputs, fin_target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00BBEZLJ-DQn","colab_type":"code","colab":{}},"source":["class BERTBaseUncased(nn.Module):\n","    def __init__(self):\n","        super(BERTBaseUncased, self).__init__()\n","        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n","        self.bert_drop = nn.Dropout(0.3)\n","        self.out = nn.Linear(768, 1)\n","        \"\"\" 768: bert we use have 768 features | 1: binary classification\n","        if we use 2, we need to change the loss function\"\"\"\n","\n","    def forward(self, ids, mask, token_type_ids):\n","        _, o2 = self.bert(\n","            ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","        \"\"\" We have 2 outputs from a BERT model\n","         o1(last hidden state): is the sequence of hidden states. eg. if we have 512 tokens (MAX_LEN), \n","         we have 512 vectors of size 768 for each batch. We can use out1 to max pooling or averge pooling\n","         o2(pooler output from bert pooler layer): we get vector of size 768 for each sample in batch\"\"\"\n","        bo = self.bert_drop(o2)                                 # drop-out\n","        output = self.out(bo)                                   # linear-layer\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQNzSGVi-DLh","colab_type":"code","colab":{}},"source":["from sklearn import metrics\n","\n","def run(index, flags):\n","\n","    flags['MAX_LEN'] = 256\n","    flags['TRAIN_BATCH_SIZE'] = 32  # 9.47min for BS=8\n","                                    # 5.54min for BS=32\n","    flags['VALID_BATCH_SIZE'] = 16\n","    flags['EPOCHS'] = 10\n","    flags['seed'] = 1234  \n","    torch.manual_seed(flags['seed'])\n","\n","    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n","    dfx.sentiment = dfx.sentiment.apply(  # can use label encoding\n","        lambda x: 1 if x == \"positive\" else 0  # can use map fn\n","    )\n","\n","    df_train, df_valid = model_selection.train_test_split(\n","        dfx,\n","        test_size=0.1,\n","        random_state=42,\n","        stratify=dfx.sentiment.values  # when split both train and val have same positive to negative sample ratio\n","    )\n","\n","    df_train = df_train.reset_index(drop=True)  # 0 to length of df_train\n","    df_valid = df_valid.reset_index(drop=True)  # 0 to length of df_valid\n","\n","    train_dataset = BERTDataset(\n","        review=df_train.review.values,\n","        target=df_train.sentiment.values\n","    )\n","    train_sampler = torch.utils.data.DistributedSampler(\n","       train_dataset,\n","       num_replicas=xm.xrt_world_size(),\n","       rank=xm.get_ordinal(),\n","       shuffle=True \n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=flags['TRAIN_BATCH_SIZE'],\n","        sampler=train_sampler\n","    )\n","####################################################\n","    valid_dataset = BERTDataset(\n","        review=df_valid.review.values,\n","        target=df_valid.sentiment.values\n","    )\n","    valid_sampler = torch.utils.data.DistributedSampler(\n","       valid_dataset,\n","       num_replicas=xm.xrt_world_size(),\n","       rank=xm.get_ordinal()\n","    )\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=flags['VALID_BATCH_SIZE'],\n","        sampler=valid_sampler\n","    )\n","\n","    # device = \"cuda\"                   # cuda\n","    device = xm.xla_device()            # tpu\n","\n","    model = BERTBaseUncased().to(device)\n","    # lr = 3e-5 *xm.xrt_world_size()\n","    param_optimizer = list(model.named_parameters())  # specify parameters to train\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    \"\"\" These parameters are adjustable, we should take a look at different layers and\n","    the decay we want, how much learning rate etc.\"\"\"\n","\n","    num_train_steps = int(len(df_train) / flags['TRAIN_BATCH_SIZE'] / xm.xrt_world_size() * flags['EPOCHS'])\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_train_steps\n","    )\n","\n","    # model = nn.DataParallel(model)              # converting to multi gpu model\n","\n","    best_accuracy = 0\n","    for epoch in tqdm(range(flags['EPOCHS']), total=flags['EPOCHS']):\n","\n","        para_loader = pl.ParallelLoader(train_data_loader, [device])\n","        train_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler)\n","\n","        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n","        outputs, target = eval_fn(para_loader.per_device_loader(device), model, device)\n","        outputs = np.array(outputs) >= 0.5\n","        accuracy = metrics.accuracy_score(target, outputs)\n","        xm.master_print(f\"Accuracy score = {accuracy}\")\n","        xm.save(model.state_dict(), MODEL_PATH)          # tpu  # saving the model only if it improves\n","\n","\n","# flags = {}\n","# xmp.spawn(run, args=(flags,), nprocs=8, start_method='fork')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLToZpyj4wNC","colab_type":"code","colab":{}},"source":["cd /content/drive/My Drive/Colab Notebooks/BERT_training_sentiment_model_using_GPU_&_TPU_IMDB_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFq8lNiK-DJd","colab_type":"code","colab":{}},"source":["!pip install -U Flask\n","!pip install flask-ngrok"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kdj7YKV_N9n3","colab_type":"code","colab":{}},"source":["import flask\n","from flask import Flask\n","from flask import request, render_template\n","\n","import torch\n","import flask\n","import time\n","from flask import Flask\n","from flask import request\n","\n","import functools        # LRU -Least Recently Used. Keeps only the most used results and deletes the least used cache\n","\n","import torch.nn as nn\n","import joblib\n","\n","from flask_ngrok import run_with_ngrok\n","\n","app = Flask(__name__)     # initialize flask app\n","run_with_ngrok(app)\n","MODEL = None\n","DEVICE = xm.xla_device()\n","PREDICTION_DICT = dict()\n","memory = joblib.Memory(\"../input/\", verbose=0)\n","\n","@app.route(\"/\")\n","def home():\n","  return render_template(\"index.html\")\n","\n","def predict_from_cache(sentence):         \n","    if sentence in PREDICTION_DICT:\n","        return PREDICTION_DICT[sentence]\n","    else:\n","        result = sentence_prediction(sentence)\n","        PREDICTION_DICT[sentence] = result\n","        return result\n","\"\"\" we create a cache file to save the previous results. This significantly reduces the time \n","taken by our model API to find the results\"\"\"\n","\n","@memory.cache\n","def sentence_prediction(sentence):\n","    tokenizer = TOKENIZER\n","    max_len = MAX_LEN\n","    review = str(sentence)\n","    review = \" \".join(review.split())     # removes all unnecessary space\n","\n","    inputs = tokenizer.encode_plus(       # encode_plus can encode 2 strings at a time\n","        review,\n","        None,                             # since we use only 1 string at a time\n","        add_special_tokens=True,          # adds cls, sep tokens\n","        max_length=max_len,\n","        truncation=True\n","    )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]      # since only 1 string token_type_ids are same and unnecessary\n","\n","    padding_length = max_len - len(ids)            # for bert we pad on the right side\n","    ids = ids + ([0] * padding_length)             # zero times the padding length\n","    mask = mask + ([0] * padding_length)\n","    token_type_ids = token_type_ids + ([0] * padding_length)\n","\n","    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)    # since dataloader always returns batches\n","    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n","    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n","\n","    ids = ids.to(DEVICE, dtype=torch.long)          # send to tpu DEVICE\n","    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n","    mask = mask.to(DEVICE, dtype=torch.long)\n","\n","    outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","\n","    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n","    return outputs[0][0]          # since outputs will be 2-Dimensional but there is only 1 value\n","\n","\n","@app.route(\"/predict\", methods=['POST'])            # creating end point using flask\n","def predict():                                      # predict function\n","    sentence = [str(x) for x in request.form.values()]\n","    sentence = sentence[0]\n","    # sentence = str(request.form.values())\n","    start_time = time.time()\n","    positive_prediction = sentence_prediction(sentence)\n","    negative_prediction = 1 - positive_prediction\n","    response = {}\n","    response[\"response\"] = {\n","        \"positive\": str(positive_prediction),\n","        \"negative\": str(negative_prediction),\n","        \"sentence\": str(sentence),\n","        \"time_taken\": str(time.time() - start_time),\n","    }\n","    return render_template(\"index.html\", sentence = r\"Your Sentence = '{}'\".format(sentence),\n","    prediction_text=\"Sentiment of the sentence is :\", positive=  \"{}% POSITIVE\".format(round(positive_prediction*100,2)),\n","    negative = \"{}% NEGATIVE\".format(round(negative_prediction*100,2)))\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    MODEL = BERTBaseUncased()\n","    MODEL.load_state_dict(torch.load(MODEL_PATH))\n","    MODEL.to(DEVICE)\n","    MODEL.eval()\n","    app.run()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TaHk87S-Zz_","colab_type":"code","colab":{}},"source":["# app = Flask(__name__)     # initialize flask app\n","# run_with_ngrok(app)       # ngrok for colab VM\n","\n","# MODEL = None\n","# DEVICE = xm.xla_device()\n","# PREDICTION_DICT = dict()\n","# memory = joblib.Memory(\"../input/\", verbose=0)\n","\n","\n","# def predict_from_cache(sentence):\n","#     if sentence in PREDICTION_DICT:\n","#         return PREDICTION_DICT[sentence]\n","#     else:\n","#         result = sentence_prediction(sentence)\n","#         PREDICTION_DICT[sentence] = result\n","#         return result\n","\n","\n","# @memory.cache\n","# def sentence_prediction(sentence):\n","#     tokenizer = TOKENIZER\n","#     max_len = MAX_LEN\n","#     review = str(sentence)\n","#     review = \" \".join(review.split())     # removes all unnecessary space\n","\n","#     inputs = tokenizer.encode_plus(       # encode_plus can encode 2 strings at a time\n","#         review, \n","#         None,                             # since we use only 1 string at a time\n","#         add_special_tokens=True,          # adds cls, sep tokens\n","#         max_length=max_len\n","#     )\n","\n","#     ids = inputs[\"input_ids\"]\n","#     mask = inputs[\"attention_mask\"]\n","#     token_type_ids = inputs[\"token_type_ids\"]      # since only 1 string token_type_ids are same and unnecessary\n","\n","#     padding_length = max_len - len(ids)            # for bert we pad on the right side\n","#     ids = ids + ([0] * padding_length)             # zero times the padding length\n","#     mask = mask + ([0] * padding_length)\n","#     token_type_ids = token_type_ids + ([0] * padding_length)\n","\n","#     ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)    # since dataloader always returns batches\n","#     mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n","#     token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n","\n","#     ids = ids.to(DEVICE, dtype=torch.long)          # send to tpu DEVICE\n","#     token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n","#     mask = mask.to(DEVICE, dtype=torch.long)\n","\n","#     outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","\n","#     outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n","#     return outputs[0][0]          # since outputs will be 2-Dimensional but there is only 1 value\n","\n","\n","# @app.route(\"/predict\")            # creating end point using flask\n","# def predict():                    # predict function\n","#     sentence = request.args.get(\"sentence\")\n","#     start_time = time.time()\n","#     positive_prediction = sentence_prediction(sentence)\n","#     negative_prediction = 1 - positive_prediction\n","#     response = {}\n","#     response[\"response\"] = {\n","#         \"positive\": str(positive_prediction),\n","#         \"negative\": str(negative_prediction),\n","#         \"sentence\": str(sentence),\n","#         \"time_taken\": str(time.time() - start_time),\n","#     }\n","#     return flask.jsonify(response)\n","    \n","\n","\n","# if __name__ == \"__main__\":\n","#     MODEL = BERTBaseUncased()\n","#     # MODEL = nn.DataParallel(MODEL)      # for multicore parallel training\n","#     MODEL.load_state_dict(torch.load(MODEL_PATH))\n","#     MODEL.to(DEVICE)\n","#     MODEL.eval()\n","#     app.run()"],"execution_count":null,"outputs":[]}]}